# Visual Testing Detailed Methods Reference

> Complete methodology for each visual design testing method

---

## Part 1: 5-Second Test (Full Method)

### Purpose
Capture users' "gut reaction" or first impression of a design. Five seconds is enough to form an impression of visual style but too short for reading details.

### When to Use
- Validating overall visual direction
- Testing landing page effectiveness
- Checking if key messages are noticed
- Evaluating brand perception at first glance

### How to Conduct

**Preparation**:
- Select the screen/design to test
- Prepare 3-5 follow-up questions
- Decide on moderated vs unmoderated format
- Recruit 5-15 participants per design

**Important Rule**: Do NOT warn participants that the design will only be shown for 5 seconds. This primes them to memorize rather than react naturally.

**Session Flow**:
1. Provide brief context if needed (e.g., "This is a banking website")
2. Display the design for exactly 5 seconds
3. Hide the design
4. Ask follow-up questions immediately

**Follow-Up Questions**:
- What do you remember seeing?
- What do you think this company/product does?
- What three words would you use to describe what you saw?
- What stood out to you most?
- How did the design make you feel?
- Would you trust this company? Why or why not?

### Analyzing Results
- Look for patterns in what participants remember
- Note which elements were most noticed (good for "One Highlight" validation)
- Categorize descriptive words as positive, negative, or neutral
- Compare against your intended brand traits

### Staylook-Specific Questions
- Did participants notice THE one highlight (Expressive element)?
- Did the curved aesthetic create intended warmth/friendliness?
- Was the visual hierarchy clear (what stood out first, second)?
- Did the minimal approach feel premium or empty?

---

## Part 2: First-Click Test (Full Method)

### Purpose
Determine if users can quickly find what they need based on visual design. Tests visual hierarchy, information architecture visibility, and CTA effectiveness.

### When to Use
- Testing navigation visibility
- Evaluating CTA placement and prominence
- Validating visual hierarchy guides users correctly
- Checking if important features are findable

### How to Conduct

**Preparation**:
- Define 3-5 specific tasks users should find
- Prepare static screenshots of the interface
- Decide on click tracking method
- Recruit 8-12 participants per design

**Session Flow**:
1. Present the design (static image)
2. Give a specific task: "Find where you would click to [action]"
3. Record the first click location
4. Stop after first click (don't let them browse)
5. Repeat for each task

**Example Tasks**:
- "Where would you click to create a new project?"
- "Find where you would access your account settings"
- "Locate the main action you can take on this page"

### Analyzing Results
- Calculate success rate per task (clicked correct area)
- Create click heat maps showing where users clicked
- Identify areas of confusion (scattered clicks)
- Note if "One Highlight" (Expressive element) draws clicks when intended

### Staylook-Specific Analysis
- Did the Expressive button attract clicks for main actions?
- Did Standard buttons get clicked for secondary actions?
- Did radius hierarchy create expected visual grouping?
- Were users drawn to the correct focal point?

---

## Part 3: Preference Test (Full Method)

### Purpose
Compare two or three design variations to understand which resonates better with users and why.

### When to Use
- Choosing between design directions
- Validating design decisions with data
- Testing specific element variations (color, layout, etc.)
- Narrowing down from multiple concepts

### How to Conduct

**Preparation**:
- Limit to 2-3 design variations maximum
- Ensure differences are significant (not subtle font changes)
- Change only one major element between versions to isolate impact
- Prepare probing questions
- Recruit 8-15 participants

**Key Rule**: Differences must be noticeable to non-designers. Subtle variations (minor font size, similar colors) will confuse participants.

**Key Rule**: Randomize or counterbalance the order of versions shown to each participant. First-seen designs can bias responses.

**Session Flow**:
1. Show first design variation
2. Allow participant to study it (not timed)
3. Show second design variation
4. Ask which they prefer and why
5. Probe for specific reasons

**Preference Questions**:
- Which design do you prefer? Why?
- Which design feels more [trustworthy/professional/modern]?
- Which design would you expect from a [type of company]?
- What specifically influenced your choice?

### Staylook-Specific Questions
- Which design feels more premium?
- Which design's highlight (Expressive element) is more effective?
- Which design has better visual balance?
- Which curved aesthetic feels more appropriate?

---

## Part 4: Desirability Testing (Full Method)

### Purpose
Verify specific brand traits are perceived using a predefined list of words.

### The Microsoft Desirability Toolkit Method

**Constructing the Word List**:

Include these categories (total 20-30 words):
1. **Target Brand Traits** (3-5 words): Words you WANT users to choose
2. **Opposite Traits**: Contradictory/negative qualities
3. **Distractor Words**: Other possible attributes (not target, not negative)

**Staylook Word List Example**:

| Target Traits | Opposite Traits | Distractors |
|---------------|-----------------|-------------|
| Premium | Cheap | Professional |
| Curved | Sharp | Modern |
| Minimal | Cluttered | Corporate |
| Expressive | Boring | Functional |
| Friendly | Cold | Elegant |
| Trustworthy | Suspicious | Simple |
| Editorial | Messy | Traditional |
| Refined | Rough | Bold |

### Session Flow
1. Show the design
2. Present the randomized word list
3. Ask: "Choose 3-5 words that best describe this design"
4. Allow participant to view design while choosing (important)
5. Ask why they chose each word

**Important**: Randomize word order for each participant.
**Important**: Don't combine with 5-second test—participants won't remember.

### Analyzing Results
- Count frequency of each word selected
- Calculate percentage of participants choosing target traits
- Compare target traits vs opposite traits
- Identify any unexpected popular descriptors

---

## Part 5: Rating Scales (Full Method)

### Purpose
Get quantitative data on specific design qualities for statistical comparison.

### When to Use
- Need quantitative data
- Want to track perception over time
- Comparing designs with statistical rigor
- Have larger participant pool (15+)

### How to Structure

**Choose 3-5 Brand Traits to Rate**:
- Focus on most important qualities
- Use qualities validated in earlier qualitative research
- Don't measure too many—keeps questionnaire short

**Scale Format**:
- Use 5-point or 7-point scales
- Label the endpoints clearly
- Use semantic differential (opposite traits at each end)

**Staylook Rating Scales**:

| Quality | 1 | 2 | 3 | 4 | 5 |
|---------|---|---|---|---|---|
| Cheap ←→ Premium | ○ | ○ | ○ | ○ | ○ |
| Sharp ←→ Curved | ○ | ○ | ○ | ○ | ○ |
| Cluttered ←→ Minimal | ○ | ○ | ○ | ○ | ○ |
| Boring ←→ Expressive | ○ | ○ | ○ | ○ | ○ |
| Cold ←→ Friendly | ○ | ○ | ○ | ○ | ○ |
| Amateur ←→ Professional | ○ | ○ | ○ | ○ | ○ |
| Confusing ←→ Clear | ○ | ○ | ○ | ○ | ○ |

### Analyzing Results
- Calculate mean and median for each quality
- Calculate standard deviation (consistency of perception)
- Compare scores across design variations
- Track scores over design iterations
- Use statistical tests for comparison (t-test, ANOVA)

---

## Part 6: A/B Testing (Full Method)

### Purpose
Compare two design versions in production to measure real behavioral impact.

### When to Use
- Measuring conversion impact of visual changes
- Testing specific element variations
- Need statistical confidence in design decisions
- Have sufficient traffic for statistical significance

### What to A/B Test

**Visual Elements Worth Testing**:
- CTA button color (Expressive vs Standard)
- Layout variations
- Typography choices
- Image styles
- Card vs list layouts
- Hero section designs

**Metrics to Measure**:
- Click-through rate on key CTAs
- Conversion rate
- Time on page
- Scroll depth
- Bounce rate
- Task completion rate

### A/B Testing Guidelines
- Change one major element at a time
- Run test until statistical significance reached
- Ensure adequate sample size (use A/B test calculators)
- Test during representative time periods
- Document what was changed and why

### Staylook A/B Test Ideas
- Expressive button vs Standard button for main CTA
- Different radius sizes
- Different "One Highlight" placements
- Intensity scale variations (Muted vs Calm backgrounds)
- Typography contrast levels

---

## Part 7: Eyetracking (Full Method)

### Purpose
Track where users look and for how long to understand which elements attract attention.

### What Eyetracking Reveals
- **Fixations**: Where users focus attention
- **Gaze paths**: Order of elements noticed
- **Heat maps**: Cumulative attention patterns
- **Areas of interest (AOI)**: Key element attention metrics

### Key Questions to Answer
- Does the "One Highlight" (Expressive element) attract first attention?
- Is visual hierarchy guiding eyes in the right order?
- Are important elements being noticed?
- Are users distracted by non-essential elements?

### Alternatives to Eyetracking Equipment
- First-click testing (approximates attention)
- Ask participants to describe what they noticed
- Think-aloud protocols during viewing
- Post-viewing recall questions

### Staylook Eyetracking Questions
- Does the single Expressive element draw first fixation?
- Do radius-nested containers create expected grouping?
- Is the Standard vs Expressive distinction creating hierarchy?
- Does the curved aesthetic affect scan patterns?

---

## Part 8: Analysis & Reporting

### Analyzing Qualitative Feedback

**Process for Open-Ended Responses**:
1. Collect All Responses: Compile in spreadsheet
2. Read Through All: Get overall sense of themes
3. Code Responses: Apply labels/tags to each response
4. Group by Theme: Cluster similar responses together
5. Count Frequency: Quantify how often themes appear
6. Compare to Goals: Check against target brand traits
7. Identify Gaps: Note missing or opposite perceptions

### Analyzing Quantitative Data

**For Closed Word Choice**:
- Calculate percentage choosing each word
- Create bar chart of word frequency
- Highlight target traits vs actual selections
- Statistical comparison if testing multiple designs

**For Rating Scales**:
- Calculate mean, median, mode for each quality
- Calculate standard deviation (consensus level)
- Create radar/spider chart of all qualities
- Statistical tests (t-test) for comparing designs

### Report Structure

**Executive Summary**:
- Purpose of testing
- Key findings (3-5 bullet points)
- Recommendations

**Methodology**:
- Methods used
- Participants (number, demographics)
- Designs tested

**Findings by Method**:
- Results from each testing method
- Supporting data and visualizations
- Quotes and examples

**Brand Perception Analysis**:
- Target traits vs perceived traits
- Gaps identified
- Competitor comparison (if applicable)

**Recommendations**:
- Prioritized list of design changes
- Rationale for each recommendation
- Expected impact

---

*Visual Testing Detailed Methods — Complete Reference*
